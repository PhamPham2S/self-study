# ν”„λ΅μ νΈ: ν•κµ­μ–΄ LLM ν¨μ¨μ  λ―Έμ„Έ μ΅°μ • (Full Fine-tuning vs LoRA)

> **ν•µμ‹¬ κ°€μ„¤:** "LoRA(Low-Rank Adaptation)λ¥Ό μ‚¬μ©ν•λ©΄ κΈ°μ΅΄ Full Fine-tuning λ€λΉ„ ν•™μµ νλΌλ―Έν„°λ¥Ό 99% μ΄μƒ μ¤„μ΄λ©΄μ„λ„, λ™μΌν• μμ¤€μ ν•κµ­μ–΄ κ°μ • λ¶„μ„ μ„±λ¥μ„ λ‹¬μ„±ν•  μ μμΌλ©° λ¨λΈμ μ•μ •μ„±(Robustness)μ€ λ” λ›°μ–΄λ‚  κ²ƒμ΄λ‹¤."

## 1. ν”„λ΅μ νΈ κ°μ” (Overview)

κ±°λ€ μ–Έμ–΄ λ¨λΈ(LLM)μ„ νΉμ • λ„λ©”μΈμ— λ§κ² λ―Έμ„Έ μ΅°μ •(Fine-tuning)ν•λ” κ²ƒμ€ λ§‰λ€ν• GPU μμ›κ³Ό μ‹κ°„μ„ μ”κµ¬ν•©λ‹λ‹¤. λ³Έ ν”„λ΅μ νΈλ” ν•κµ­μ–΄ λ²¤μΉλ§ν¬ λ¨λΈμΈ `klue/roberta-base`λ¥Ό λ€μƒμΌλ΅, λ„¤μ΄λ²„ μν™” λ¦¬λ·° λ°μ΄ν„°μ…‹(NSMC) κ°μ • λ¶„λ¥ κ³Όμ λ¥Ό μν–‰ν•¨μ— μμ–΄ **μ „ν†µμ μΈ Full Fine-tuning λ°©μ‹**κ³Ό **μµμ‹  κ²½λ‰ν™” κΈ°λ²•μΈ LoRA(PEFT)** λ°©μ‹μ ν¨μ¨μ„± λ° μ„±λ¥μ„ μ •λ‰μ μΌλ΅ λΉ„κµ λ¶„μ„ν•©λ‹λ‹¤.

### β¨ ν•µμ‹¬ μ„±κ³Ό μ”μ•½
- **μ••λ„μ  ν¨μ¨μ„±:** LoRA μ μ© μ‹ ν•™μµ νλΌλ―Έν„° μ **99.2% κ°μ†**, VRAM μ‚¬μ©λ‰ **μ•½ 40% μ κ°**.
- **λ™λ“±ν• μ„±λ¥:** Full Fine-tuning λ€λΉ„ μ •ν™•λ„ μ°¨μ΄λ” **0.17%p**λ΅ μ‚¬μ‹¤μƒ λ™μΌν• μ„±λ¥ λ‹¬μ„±.
- **μ•μ •μ„± ν™•λ³΄:** Full Fine-tuningμ—μ„ λ°μƒν• **κ³Όμ ν•©(Overfitting) ν„μƒμ„ LoRAκ°€ ν¨κ³Όμ μΌλ΅ μ–µμ **ν•μ—¬ λ” μΌλ°ν™”λ λ¨λΈ ν™•λ³΄.

<br>

## 2. μ‹¤ν— ν™κ²½ λ° μ„¤κ³„ (Experimental Setup)

κ³µμ •ν• λΉ„κµλ¥Ό μ„ν•΄ λ‘ μ‹¤ν—κµ°μ€ **λ™μΌν• λ°μ΄ν„°μ…‹, μ‹λ“(Seed), λ°°μΉ ν¬κΈ°, ν•™μµ μ—ν­** ν™κ²½μ—μ„ ν†µμ λμ—μµλ‹λ‹¤.

| ν•­λ© | λ‚΄μ© | λΉ„κ³  |
| --- | --- | --- |
| **Base Model** | `klue/roberta-base` | μ•½ 1.1μ–µ κ° νλΌλ―Έν„° |
| **Dataset** | NSMC (Naver Sentiment Movie Corpus) | Train(15λ§), Test(5λ§) |
| **Task** | Sequence Classification (Binary) | κΈμ •(1) / λ¶€μ •(0) |
| **GPU** | NVIDIA A100 / V100 κΈ‰ (Colab Pro) | λ‹¨μΌ GPU ν™κ²½ |
| **Seed κ³ μ •** | `42` | μ¬ν„μ„± ν™•λ³΄λ¥Ό μ„ν• ν•„μ ν†µμ  λ³€μΈ |
| **Hyperparams** | Batch: 32, Epochs: 3, FP16: True | ν•™μµλ¥ (LR)μ€ κ° λ°©μ‹μ— λ§κ² μµμ ν™” |

### π§ λΉ„κµκµ° μ„¤μ •
1.  **λ€μ΅°κµ° (Baseline): Full Fine-tuning**
    - λ¨λΈμ λ¨λ“  νλΌλ―Έν„°(μ•½ 1.1μ–µ κ°)λ¥Ό μ—…λ°μ΄νΈν•©λ‹λ‹¤.
    - ν•™μµλ¥ (LR): `2e-5`

2.  **μ‹¤ν—κµ° (Experiment): LoRA (PEFT)**
    - Backbone λ¨λΈμ„ μ–Όλ¦¬κ³ (Freeze), Attention Layerμ Query, Valueμ—λ§ μ–΄λ‘ν„°λ¥Ό λ¶€μ°©ν•μ—¬ ν•™μµν•©λ‹λ‹¤.
    - μ„¤μ •: Rank(`r`)=8, Alpha=32, Target Modules=`["query", "value"]`
    - ν•™μµλ¥ (LR): `2e-4` (νλΌλ―Έν„°κ°€ μ μ–΄ λ” λ†’μ€ LR ν•„μ”)

<br>

## 3. κ²°κ³Ό λ¶„μ„ (Results & Analysis)

### π“ μΆ…ν•© λ¶„μ„ λ€μ‹λ³΄λ“

μ•„λ μ΄λ―Έμ§€λ” λ‘ μ‹¤ν— λ°©μ‹μ νλΌλ―Έν„° μ, ν•™μµ μ†μ‹¤ κ³΅μ„ (Loss Curve), κ·Έλ¦¬κ³  μµμΆ… μ„±λ¥ μ§€ν‘λ¥Ό μΆ…ν•©μ μΌλ΅ μ‹κ°ν™”ν• κ²°κ³Όμ…λ‹λ‹¤.

![Final Result](./output.png)
*(μ°Έκ³ : μ„ μ΄λ―Έμ§€λ” ν”„λ΅μ νΈ μ‹¤ν–‰ κ²°κ³Όλ΅ μƒμ„±λ μ‹¤μ  λ°μ΄ν„° κΈ°λ° μ‹κ°ν™” μλ£μ…λ‹λ‹¤.)*

### π§ μƒμ„Έ λ¶„μ„

#### 1) ν¨μ¨μ„± (Efficiency): LoRAμ μ••μΉ
* **νλΌλ―Έν„° μ:** 1.1μ–µ κ° vs **88λ§ κ°**. λ΅κ·Έ μ¤μΌ€μΌ(Log Scale) μ°¨νΈμ—μ„ λ³΄λ“― μ••λ„μ μΈ μ°¨μ΄μ…λ‹λ‹¤. μ΄λ” λ¨λΈ μ¤ν† λ¦¬μ§€ μ©λ‰μ νκΈ°μ  κ°μ†(~420MB β†’ ~3MB)λ΅ μ΄μ–΄μ§‘λ‹λ‹¤.
* **λ¦¬μ†μ¤ μ†λ¨:** VRAMμ€ **μ•½ 1.2GB μ μ•½**(3.02GB β†’ 1.85GB)λμ—μΌλ©°, ν•™μµ μ‹κ°„μ€ μ•½ **27% λ‹¨μ¶•**λμ—μµλ‹λ‹¤. μ΄λ” LoRAκ°€ μ ν•λ GPU ν™κ²½μ—μ„ κ±°λ€ λ¨λΈμ„ λ‹¤λ£¨λ” λ° ν•„μμ μ„μ„ μ‹μ‚¬ν•©λ‹λ‹¤.

#### 2) μ„±λ¥ λ° μ•μ •μ„± (Performance & Robustness): LoRAμ μ‹¤μ§μ  μ°μ„
* **λ‹¨μ μ •ν™•λ„ (Accuracy):** Full FT(90.56%)κ°€ LoRA(90.39%)λ³΄λ‹¤ 0.17%p λ†’κ² λ‚νƒ€λ‚¬μµλ‹λ‹¤. ν•μ§€λ§ μ΄λ” ν†µκ³„μ μΌλ΅ μ μλ―Έν• μ°¨μ΄κ°€ μ•„λ‹ μ¤μ°¨ λ²”μ„ λ‚΄ μμ¤€μ…λ‹λ‹¤.
* **κ³Όμ ν•©(Overfitting) μ§•ν›„ ν¬μ°© (ν•µμ‹¬):** Loss Curveμ™€ ν•λ‹¨ ν‘μ **'Generalization Gap'**μ„ μ£Όλ©ν•΄μ•Ό ν•©λ‹λ‹¤.
    * **Full FT (λΉ¨κ°„ μ μ„ ):** Train Lossλ” λ§¤μ° λ‚®μ•„μ§€μ§€λ§, Validation Lossλ” νΉμ • μ‹μ  μ΄ν›„ κ°μ†λ¥Ό λ©μ¶”κ³  μ •μ²΄λ©λ‹λ‹¤. Gapμ΄ **0.1618**λ΅ λ§¤μ° μ»¤, ν•™μµ λ°μ΄ν„°μ— κ³Όν•κ² μµμ ν™”(Overfitting)λ μƒνƒμ…λ‹λ‹¤.
    * **LoRA (νλ€ μ μ„ ):** Train Lossμ™€ Validation Lossκ°€ μ•μ •μ μΌλ΅ ν•¨κ» κ°μ†ν•©λ‹λ‹¤. Gapμ΄ **0.0154**λ΅ λ§¤μ° μ‘μ•„, μƒλ΅μ΄ λ°μ΄ν„°μ— λ€ν• μΌλ°ν™” μ„±λ¥(Robustness)μ΄ λ›°μ–΄λ‚©λ‹λ‹¤.

<br>

## 4. κ²°λ΅  (Conclusion)

λ³Έ ν”„λ΅μ νΈλ¥Ό ν†µν•΄ **LoRAλ” Full Fine-tuningμ μ™„λ²½ν• λ€μ²΄μ¬**κ°€ λ  μ μμμ„ μ‹¤ν—μ μΌλ΅ μ¦λ…ν–μµλ‹λ‹¤.

λ‹¨μν GPU λ©”λ¨λ¦¬λ¥Ό μ•„λΌλ” κ²ƒμ„ λ„μ–΄, μ μ€ μμ νλΌλ―Έν„°λ§ ν•™μµν•¨μΌλ΅μ¨ κ±°λ€ λ¨λΈμ΄ ν•™μµ λ°μ΄ν„°μ λ…Έμ΄μ¦κΉμ§€ μ™Έμ›λ²„λ¦¬λ” **κ³Όμ ν•© λ¬Έμ λ¥Ό ν¨κ³Όμ μΌλ΅ λ°©μ§€**ν•  μ μμ—μµλ‹λ‹¤. κ²°λ΅ μ μΌλ΅, μ‹¤μ  μ„λΉ„μ¤ λ°°ν¬ λ° μ΄μ κ΄€μ μ—μ„λ” μ•½κ°„μ μ •ν™•λ„ μ΄λ“μ„ μ„ν•΄ λ¶μ•μ •ν• Full Fine-tuningμ„ μ„ νƒν•λ” κ²ƒλ³΄λ‹¤, **ν¨μ¨μ μ΄κ³  μ•μ •μ μΈ LoRAλ¥Ό μ„ νƒν•λ” κ²ƒμ΄ ν•©λ¦¬μ **μ…λ‹λ‹¤.

<br>

## 5. μ‚¬μ© λ°©λ²• (How to Run)

μ΄ ν”„λ΅μ νΈλ” Google Colab ν™κ²½μ—μ„ μ‹¤ν–‰λλ„λ΅ μ„¤κ³„λμ—μµλ‹λ‹¤.

**ν•„μ λΌμ΄λΈλ¬λ¦¬ μ„¤μΉ:**
```bash
!pip install transformers datasets evaluate peft accelerate