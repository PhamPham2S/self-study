{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65bfd251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b0556c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# 1. 필수 라이브러리 설치\n",
    "# transformers: 모델 및 토크나이저 로드\n",
    "# datasets: 데이터셋 다운로드 및 처리\n",
    "# evaluate: 성능 평가 (Accuracy, F1 등)\n",
    "# peft: 향후 사용할 경량화 튜닝 라이브러리\n",
    "# accelerate: 학습 가속화 도구\n",
    "!pip install -q transformers datasets evaluate peft accelerate scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "663d8cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8101304d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: klue, ynat...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1442e2ac24674a9bab35a074f6230fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d022d54058c84d66bba289f045c099a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ynat/train-00000-of-00001.parquet:   0%|          | 0.00/4.17M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c7580b7e8e4b969cb0b28a85de40ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ynat/validation-00000-of-00001.parquet:   0%|          | 0.00/847k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de6085276e94413b3ce90cfa1af166d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/45678 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce4d3c703fa4cc088164f347e6c3a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/9107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 45678\n",
      "Example: {'guid': 'ynat-v1_train_00000', 'title': '유튜브 내달 2일까지 크리에이터 지원 공간 운영', 'label': 3, 'url': 'https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=105&sid2=227&oid=001&aid=0008508947', 'date': '2016.06.30. 오전 10:36'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 1. 설정 (Config)\n",
    "MODEL_ID = \"klue/roberta-base\" # 한국어 성능이 좋은 RoBERTa 모델 사용\n",
    "\n",
    "# 2. 데이터셋 로드\n",
    "print(\"Loading dataset: klue, ynat...\")\n",
    "# 'klue'는 데이터셋 이름, 'ynat'는 그 안의 세부 주제(Task) 이름입니다.\n",
    "dataset = load_dataset(\"klue\", \"ynat\")\n",
    "\n",
    "# 데이터 확인 (선택 사항)\n",
    "print(f\"Train data size: {len(dataset['train'])}\")\n",
    "print(f\"Example: {dataset['train'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e174f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec534a8c511c451c9c34f243a2f96836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45678 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f59053668bd411fb59dd47de8c657be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9107 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation complete.\n"
     ]
    }
   ],
   "source": [
    "# 3. 토크나이저 로드 및 전처리 함수 정의\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # 제목(title)을 입력으로 받아 토큰화\n",
    "    return tokenizer(examples[\"title\"], truncation=True, padding=True)\n",
    "\n",
    "# 4. 전체 데이터셋에 전처리 적용 (Map)\n",
    "print(\"Tokenizing data...\")\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\") # label -> labels 로 변경\n",
    "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# 데이터 분류 (Train / Validation) - ynat은 validation이 이미 나뉘어 있음\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]\n",
    "\n",
    "# Data Collator (배치 단위로 패딩 맞춤)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "print(\"Data preparation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66bfe9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da664c9378414f0898e64f9805d79a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/546 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af0a5b20e4a4dc7893efd9dc6eb84cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Feature Extraction Mode (Backbone Freezed)\n",
      "trainable params: 595975 || all params: 110623495 || trainable%: 0.54\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "\n",
    "# 1. 모델 로드\n",
    "# klue/ynat은 라벨이 7개입니다. (num_labels=7)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID, num_labels=7)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# [핵심] Feature Extraction을 위해 Backbone 파라미터 얼리기 (Freeze)\n",
    "# ---------------------------------------------------------\n",
    "for name, param in model.base_model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 얼려졌는지 확인 (학습 가능한 파라미터 수 출력)\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )\n",
    "\n",
    "print(\">>> Feature Extraction Mode (Backbone Freezed)\")\n",
    "print_trainable_parameters(model)\n",
    "# 결과 예시: trainable%가 1% 미만이어야 정상 (분류기 층만 학습하므로)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e6dbdf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ef2b78360a48f0ad496c09cad137b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59b56cfc6f344c58433e425e4e4473e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. 평가 지표 정의 (Accuracy, F1)\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    acc = accuracy.compute(predictions=predictions, references=labels)\n",
    "    f1_score = f1.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    return {\"accuracy\": acc[\"accuracy\"], \"f1\": f1_score[\"f1\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8a7a97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
